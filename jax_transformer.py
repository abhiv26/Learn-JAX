# -*- coding: utf-8 -*-
"""Jax Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10VBYzMiXMEXu0FlBf5MEU5S5NBDLuajd
"""

!pip  install equinox

import jax
import jax.numpy as jnp
import jax.random as jr
import equinox as eqx
import einops

class MultiHeadAttention(eqx.Module):

  d_model: int
  dropout_rate: float
  WQ: eqx.nn.Linear
  WK: eqx.nn.Linear
  WV: eqx.nn.Linear
  WO: eqx.nn.Linear
  n_heads: int
  dropout: eqx.nn.Dropout

  def __init__(self, d_model, dropout_rate, n_heads):
    qkey, kkey, vkey, okey = jr.split(jr.key(42), 4)
    self.d_model = d_model
    self.dropout_rate = dropout_rate
    self.dropout = eqx.nn.Dropout(self.dropout_rate)
    self.n_heads = n_heads
    self.WQ = eqx.nn.Linear(d_model, d_model, use_bias=False, key=qkey) # original paper doesn't use bias terms
    self.WK = eqx.nn.Linear(d_model, d_model, use_bias=False, key=kkey)
    self.WV = eqx.nn.Linear(d_model, d_model, use_bias=False, key=vkey)
    self.WO = eqx.nn.Linear(d_model, d_model, use_bias=False, key=okey)

  def __call__(self, q, k, v, mask=None):

    Q = jax.vmap(self.WQ)(q) # map over token dimension since eqx linear expects 1D vectors
    K = jax.vmap(self.WK)(k)
    V = jax.vmap(self.WV)(v)

    Q = einops.rearrange(Q, 'T (H D) -> H T D', H=self.n_heads)
    K = einops.rearrange(K, 'T (H D) -> H T D', H=self.n_heads)
    V = einops.rearrange(V, 'T (H D) -> H T D', H=self.n_heads)

    score_weights = einops.einsum(Q, K, 'H Tq D, H Tk D -> H Tq Tk') / (jnp.size(K, -1) ** 0.5)

    if mask is not None:
      score_weights = jnp.where(mask, score_weights, -1e9) # prevent current tokens from looking ahead (can only look at itself and prev tokens)

    score_weights_normalized = self.dropout(jax.nn.softmax(score_weights, -1), key=jr.key(42))
    scores = einops.einsum(score_weights_normalized, V, 'H Tq Tk, H Tv D -> H Tq D')

    attention = einops.rearrange(scores, 'H T D -> T (H D)')

    output = jax.vmap(self.WO)(attention)

    return output

key = jr.key(0)
batch_size = 2
seq_len = 5
d_model = 256
n_heads = 8

mha = MultiHeadAttention(d_model=d_model, dropout_rate=0.1, n_heads=n_heads)

x = jr.normal(key, (batch_size, seq_len, d_model)) # 2, 5, 256

output = jax.vmap(mha)(x, x, x) # map over batch dimension

print(f"Input query shape: {x.shape}")
print(f"Output attention shape: {output.shape}")

assert output.shape == (batch_size, seq_len, d_model)
print("Test case passed: Output shape is correct!")

class Embedding(eqx.Module):

  d_model: int
  vocab_size: int
  embedding: eqx.nn.Embedding

  def __init__(self, d_model, vocab_size):
    self.d_model = d_model
    self.vocab_size = vocab_size
    self.embedding = eqx.nn.Embedding(num_embeddings=vocab_size, embedding_size=d_model, key=jr.key(42))

  def __call__(self, x):
    return jax.vmap(self.embedding)(x) * (self.d_model ** 0.5) # map over token dim

# Not learnable no need to register as a pytree
def PositionalEncoding(d_model, seq_len, x):

  pe = jnp.zeros((seq_len, d_model)) # Create matrix sequence length x dimensionality
  # index each position as a col vector
  pos = jnp.arange(seq_len)[:, None]
  # index each dimension as a row vector (step by 2 to handle dims in pairs)
  # key identity: a^b = exp(b * log a), for the division term 10000^2i/dmodel
  div_term = jnp.exp(jnp.arange(0, d_model, 2) * (-jnp.log(10000) / d_model))

  pe = pe.at[:, 0::2].set(jnp.sin(pos*div_term))
  pe = pe.at[:, 1::2].set(jnp.cos(pos*div_term))

  return x + pe[:x.shape[1], :] # handle varying sequence lengths

class LayerNorm(eqx.Module):

  gamma: jnp.ndarray
  beta: jnp.ndarray
  eps: float

  def __init__(self, gamma, beta, eps=1e-05):
    self.gamma = gamma
    self.beta = beta
    self.eps = eps

  def __call__(self, x):
    mean = jnp.mean(x, axis=-1, keepdims=True) # normalize over last dim
    var= jnp.var(x, axis=-1, keepdims=True)
    x = (x - mean) / jnp.sqrt(var + self.eps)
    return x * self.gamma + self.beta

class FeedForward(eqx.Module):

  linear1: eqx.nn.Linear
  linear2: eqx.nn.Linear

  def __init__(self, dropout_rate, d_model, d_ff):
    key1, key2 = jr.split(jr.key(42))

vocab_size = 1000
d_model = 512
batch_size = 4
seq_len = 10

tokens = jnp.array([[1, 2, 3], [4, 5, 6]]) # B, T
embedding = Embedding(d_model, vocab_size)
x = jax.vmap(embedding)(tokens)
x = PositionalEncoding(d_model, seq_len, x)
layernorm = LayerNorm(0.01, 0.01)
x = layernorm(x)
x